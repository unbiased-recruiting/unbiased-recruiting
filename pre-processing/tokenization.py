# -*- coding: utf-8 -*-
"""Gender_predictor.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_KBaO8BpdsMCAXJDUv7JS6NSuiop1RBM

# Libraries and Mounting drive
"""

import io
import pandas as pd
import os
import numpy as np
from google.colab import drive
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import argparse
import sys
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import tensorflow as tf

drive.mount('/content/gdrive')
os.chdir('/content/gdrive/My Drive/')





from google.colab import files
uploaded = files.upload()

"""# Data processing

## Loading data
"""

cv_dir = './CV_txt/cv_'
filenames = []
CV_ids = []
CV = {}

#Labels downloading
labels = pd.read_excel(io.BytesIO(uploaded['Label_Sex_CV (1).xlsx']))


#Downloading CVs
for idx in set(labels['id']):
    filenames.append(cv_dir + str(idx) + '.pdf.txt')
                  
for i,f in enumerate(filenames):
  try:
    with open(f, 'r') as file:
      CV[i] = file.read()
  except Exception:
    pass
  
df_cv = pd.DataFrame.from_dict(CV
                               , orient='index').reset_index()

df_cv.columns
df_cv.columns = ['id', 'cv']

df_cv.loc[:,'id'] = df_cv.loc[:,'id']+1

"""##Creating dataframe with CVs and gender labels"""

#Final dataframe with CVs and labels
df = pd.merge(df_cv
             , labels
             , on = 'id'
             , how = 'left')

#Dropping NaN values
df.dropna(inplace = True)

"""##Stop words removal"""

import nltk
import string
nltk.download('stopwords')
nltk.download('punkt')
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize 
filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n'

#Import stop words
language = 'english'
stop_words = set(stopwords.words(language)) 
punctuation = string.punctuation + '-' + '['+ '–'+ '\uf0a7'+ ']' + '•' + filters #remember to remove utf words

#Row by row tokenization
def tokenization_and_stop_words_out(text):
  x = word_tokenize(text)
  y = [w for w in x if not w in stop_words and not w in punctuation]
  return y
  
df.loc[:,'cv'] = df['cv'].apply(tokenization_and_stop_words_out)

"""## Encoding labels

Here we use to one hot encoding for encoding genders
"""

y = pd.get_dummies(df['label'])
y = y.values

"""## Splitting into train and test df"""

X =df.drop(['id', 'label'], axis =1)

from sklearn.model_selection import train_test_split

df_train, df_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)

print(len(df_train))

"""## Tokenization"""

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import string

#Tokenizer training 
filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n' #punctuation removal
num_words = 10000
len_max_seq = 840
train_values = df_train.loc[:,'cv'].tolist()
test_values = df_test.loc[:,'cv'].tolist()

tokenizer = Tokenizer(num_words = num_words, filters= filters,lower =True)
tokenizer.fit_on_texts(df_train['cv'].tolist())

#Text to sequences
X_train = tokenizer.texts_to_sequences(train_values)
X_test = tokenizer.texts_to_sequences(test_values)

#Padding sequences
X_train = pad_sequences(X_train, len_max_seq)
X_test = pad_sequences(X_test, len_max_seq)

X_train.shape

"""## Bag of words

Limitations of BOW

*Semantic meaning: the basic BOW approach does not consider the meaning of the word in the document. It completely ignores the context in which it’s used. The same word can be used in multiple places based on the context or nearby words.

*Vector size: For a large document, the vector size can be huge resulting in a lot of computation and time. You may need to ignore words based on relevance to your use case.
"""

#from sklearn.feature_extraction.text import CountVectorizer

#train_values = df_train.loc[:,'cv'].tolist()
#train_values = [" ".join(x) for x in train_values] 
##train_text = " ".join(train_values)

#test_values = df_test.loc[:,'cv'].tolist()
#test_values = [" ".join(x) for x in test_values] 

#vectorizer = CountVectorizer()
#vec = vectorizer.fit(train_values)

#X_train = vec.transform(train_values).to_array()
#X_test = vec.transform(test_values).to_array()

#print(len(X_train.toarray()[2]))

df['cv_length'] =  df['cv'].apply(lambda x :len(x))

df['cv_length'].agg(['mean', 'max', 'min'])

"""# Analysis after cleaning"""

tokenizer.word_index

tokenizer.word_counts

"""NB: See with CountVectorizer to take into account numbers of occurencies of each word"""

import matplotlib.pyplot as plt
from nltk import FreqDist


unique_sequences = [list(set(x)) for x in sequences]
print(unique_sequences)


#Vocabulary diversity distribution
plt.hist([len(x) for x in unique_sequences])
plt.show()

#Most frequently used words
fqdist = [FreqDist(x) for x in df['cv'].tolist()]


fqdist[0].plot(200)

"""Label Encoder"""

from sklearn.preprocessing import LabelBinarizer
label_enc = LabelBinarizer().fit(list(set(df['label'].values))) 
y = label_enc.transform(df['label'].values)

print(len(tokenizer.word_index))
# before 7882

"""First neuronal networks"""

from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle



# read_csv

source_train, source_test = train_test_split(df[df['set'] == 'source'], test_size=0.1, shuffle=True) #Split source dataset
target_train, target_test = train_test_split(df[df['set'] == 'target'], test_size=0.1, shuffle=True) #Split target dataset

print('Source train', source_train.shape, source_train['polarity'].mean())
print('Source test',source_test.shape, source_test['polarity'].mean())
print('Target train', target_train.shape, target_train['polarity'].mean())
print('Target test', target_test.shape, target_test['polarity'].mean())



"""# Gender learning

## Network hyperrameters
"""

DATA_SIZE = 840
NUM_HIDDEN_1 = 512
NUM_HIDDEN_2 = 512
NUM_HIDDEN_3 = 512
NUM_CLASSES = 2

"""## Neuronal network definition"""

#Functions

def init_weights(shape):
    return tf.Variable(tf.random_normal(shape, stddev = 0.01))
w_h1 = init_weights([DATA_SIZE, NUM_HIDDEN_1])
w_h2 = init_weights([NUM_HIDDEN_1, NUM_HIDDEN_2])
w_h3 = init_weights([NUM_HIDDEN_2, NUM_HIDDEN_3])
w_o = init_weights([NUM_HIDDEN_3, NUM_CLASSES])


def model(X,w_h1, w_h2, w_h3, w_o):
    h1 = tf.nn.relu(tf.matmul(X, w_h1))
    h2 = tf.nn.relu(tf.matmul(h1, w_h2))
    h3 = tf.nn.relu(tf.matmul(h2, w_h3))
    return tf.matmul(h3, w_o)

X = tf.placeholder(tf.float32, [None, DATA_SIZE])
Y = tf.placeholder(tf.float32, [None, NUM_CLASSES])

Y_p = model(X, w_h1, w_h2,w_h3, w_o)  
  
cost_function = tf.reduce_mean(
    tf.nn.softmax_cross_entropy_with_logits_v2(logits = Y_p, labels = Y))
optimization_algorithm = tf.train.AdamOptimizer(0.5)\
.minimize(cost_function)

sess = tf.InteractiveSession()
sess.run(tf.global_variables_initializer())

correct_prediction = tf.equal(tf.argmax(Y_p,1), tf.argmax(Y,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))

for epoch in range(5000):
    if epoch%500 ==0:
        train_accuracy = accuracy.eval(feed_dict = {X:X_train\
                                                    , Y:y_train})
        print("epoch: %d, training accuracy: %g"%(epoch, train_accuracy))
    optimization_algorithm.run(feed_dict = {X:X_train\
                                                    , Y:y_train})

"""First and second try : epoch: 19500, training accuracy: 0.10526

Tokenization

Hyperparameters : 
DATA_SIZE = 840
NUM_HIDDEN_1 = 256 and 512
NUM_HIDDEN_2 = 256 and 512
NUM_CLASSES = 2

Optimizer : Gradient Descent (0.5)


Third try : epoch: 19500, training accuracy: 0.568421

Tokenization

Hyperparameters : 
DATA_SIZE = 840
NUM_HIDDEN_1 = 512
NUM_HIDDEN_2 = 512
NUM_CLASSES = 2

Optimizer : AdamOptimizer(0.5)


Fourth try : epoch: 19500, training accuracy: 0.126316

Tokenization

Hyperparameters : 
DATA_SIZE = 6840
NUM_HIDDEN_1 = 256
NUM_HIDDEN_2 = 256
NUM_CLASSES = 2

Optimizer : AdamOptimizer(0.5)



Fift try : epoch: 5000, training accuracy: 0.115789
Bag of words

Hyperparameters : 
DATA_SIZE = 6840
NUM_HIDDEN_1 = 512
NUM_HIDDEN_2 = 512
NUM_CLASSES = 2

Optimizer : AdamOptimizer(0.5)



Fift try : epoch: 5000, training accuracy: 0.4

Tokenization

Hyperparameters : 
DATA_SIZE = 6840
NUM_HIDDEN_1 = 512
NUM_HIDDEN_2 = 512
NUM_HIDDEN_3 = 512
NUM_CLASSES = 2

Optimizer : AdamOptimizer(0.5)
"""

print("\n\nTest accuracy: %g"%accuracy.eval(feed_dict={X: X_test, Y: y_test}))

